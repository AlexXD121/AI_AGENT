{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sovereign-Doc Cloud Brain\\n",
                "\\n",
                "Vision-language inference using Qwen2.5-VL-7B on Google Colab T4 GPU.\\n",
                "\\n",
                "**Requirements:**\\n",
                "- GPU Runtime (T4)\\n",
                "- ngrok Auth Token\\n",
                "- HuggingFace Token (for model access)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\\n",
                "!pip install -q vllm fastapi uvicorn pyngrok python-multipart nest-asyncio loguru pillow"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\\n",
                "from google.colab import userdata\\n",
                "\\n",
                "# Set authentication tokens\\n",
                "# Get these from: ngrok.com and huggingface.co\\n",
                "os.environ['NGROK_AUTH_TOKEN'] = userdata.get('NGROK_TOKEN')\\n",
                "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\\n",
                "\\n",
                "print('âœ“ Configuration loaded')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Upload Code Files\\n",
                "\\n",
                "Upload `colab_brain/` directory with:\\n",
                "- `__init__.py`\\n",
                "- `inference.py`\\n",
                "- `server.py`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify uploaded files\\n",
                "!ls -la colab_brain/"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Start Server with ngrok Tunnel"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nest_asyncio\\n",
                "from pyngrok import ngrok\\n",
                "import uvicorn\\n",
                "import sys\\n",
                "\\n",
                "# Enable nested async (required for Colab)\\n",
                "nest_asyncio.apply()\\n",
                "\\n",
                "# Start ngrok tunnel\\n",
                "ngrok.set_auth_token(os.environ['NGROK_AUTH_TOKEN'])\\n",
                "tunnel = ngrok.connect(8000, bind_tls=True)\\n",
                "\\n",
                "print(f'\\\\nðŸš€ Cloud Brain Public URL: {tunnel.public_url}')\\n",
                "print(f'\\\\nEndpoints:')\\n",
                "print(f'  - GET  {tunnel.public_url}/health')\\n",
                "print(f'  - POST {tunnel.public_url}/analyze')\\n",
                "print(f'\\\\nStarting server...\\\\n')\\n",
                "\\n",
                "# Add colab_brain to Python path\\n",
                "sys.path.insert(0, '/content')\\n",
                "\\n",
                "# Import and run FastAPI server\\n",
                "from colab_brain.server import app\\n",
                "\\n",
                "# Run server (blocks until shutdown)\\n",
                "uvicorn.run(app, host='0.0.0.0', port=8000, log_level='info')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Testing\\n",
                "\\n",
                "Test the endpoints using `curl` or Python requests:\\n",
                "\\n",
                "```python\\n",
                "import requests\\n",
                "\\n",
                "# Health check\\n",
                "response = requests.get(f'{tunnel.public_url}/health')\\n",
                "print(response.json())\\n",
                "\\n",
                "# Analyze image\\n",
                "with open('test_image.jpg', 'rb') as f:\\n",
                "    files = {'file': f}\\n",
                "    data = {'query': 'What is in this image?'}\\n",
                "    response = requests.post(f'{tunnel.public_url}/analyze', files=files, data=data)\\n",
                "    print(response.json())\\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}