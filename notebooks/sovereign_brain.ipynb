{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß† Sovereign-Doc Cloud Brain\n",
                "\n",
                "GPU-accelerated Vision-Language AI for document analysis using Qwen2.5-VL-7B.\n",
                "\n",
                "## Requirements\n",
                "- ‚úÖ GPU Runtime (T4 or better)\n",
                "- ‚úÖ ngrok Auth Token\n",
                "- ‚úÖ HuggingFace Token\n",
                "- ‚úÖ Sovereign Access Token\n",
                "\n",
                "## Setup Instructions\n",
                "1. Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n",
                "2. Add secrets (üîë icon in sidebar):\n",
                "   - `NGROK_TOKEN`: Your ngrok token\n",
                "   - `HF_TOKEN`: Your HuggingFace token\n",
                "   - `SOVEREIGN_ACCESS_TOKEN`: `mKEHz_sxn8g3LLGqe7cTsuRBs_QEolmDkCh_sL91akM`\n",
                "3. Upload `colab_brain/` folder (3 files) to `/content/colab_brain/`\n",
                "4. Run cells in order"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ Step 1: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install -q vllm fastapi uvicorn pyngrok python-multipart nest-asyncio loguru pillow transformers\n",
                "print('‚úÖ Dependencies installed')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîë Step 2: Load Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from google.colab import userdata\n",
                "\n",
                "# Load secrets from Colab\n",
                "try:\n",
                "    os.environ['NGROK_TOKEN'] = userdata.get('NGROK_TOKEN')\n",
                "    os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
                "    os.environ['SOVEREIGN_ACCESS_TOKEN'] = userdata.get('SOVEREIGN_ACCESS_TOKEN')\n",
                "    \n",
                "    print('‚úÖ All tokens loaded successfully')\n",
                "    print(f'   - Ngrok token: {os.environ[\"NGROK_TOKEN\"][:10]}...')\n",
                "    print(f'   - HF token: {os.environ[\"HF_TOKEN\"][:10]}...')\n",
                "    print(f'   - Access token: {os.environ[\"SOVEREIGN_ACCESS_TOKEN\"][:10]}...')\n",
                "except Exception as e:\n",
                "    print(f'‚ùå Error loading secrets: {e}')\n",
                "    print('   Make sure you added all tokens in Colab Secrets (üîë icon)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìÅ Step 3: Verify Uploaded Files\n",
                "\n",
                "Upload the `colab_brain/` folder using the file browser (üìÅ icon) on the left."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check if files are uploaded\n",
                "import os\n",
                "\n",
                "required_files = [\n",
                "    'colab_brain/__init__.py',\n",
                "    'colab_brain/inference.py',\n",
                "    'colab_brain/server.py'\n",
                "]\n",
                "\n",
                "print('üìÅ Checking uploaded files...')\n",
                "all_present = True\n",
                "for file in required_files:\n",
                "    if os.path.exists(file):\n",
                "        print(f'   ‚úÖ {file}')\n",
                "    else:\n",
                "        print(f'   ‚ùå {file} - MISSING!')\n",
                "        all_present = False\n",
                "\n",
                "if all_present:\n",
                "    print('\\n‚úÖ All files present! Ready to start server.')\n",
                "else:\n",
                "    print('\\n‚ùå Some files are missing. Please upload the colab_brain/ folder.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üöÄ Step 4: Start Cloud Brain Server\n",
                "\n",
                "This will:\n",
                "1. Start ngrok tunnel on port 8000\n",
                "2. Load Qwen2.5-VL-7B model on GPU\n",
                "3. Start FastAPI server\n",
                "4. Display public URL\n",
                "\n",
                "**Keep this cell running!** The server will stop if you interrupt it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyngrok import ngrok\n",
                "import sys\n",
                "import asyncio\n",
                "\n",
                "# Start ngrok tunnel\n",
                "print('üîå Starting ngrok tunnel...')\n",
                "ngrok.set_auth_token(os.environ['NGROK_TOKEN'])\n",
                "tunnel = ngrok.connect(8000, bind_tls=True)\n",
                "\n",
                "print(f'\\n‚úÖ Cloud Brain is LIVE!')\n",
                "print(f'=' * 60)\n",
                "print(f'üåê Public URL: {tunnel.public_url}')\n",
                "print(f'=' * 60)\n",
                "print(f'\\nüì° Endpoints:')\n",
                "print(f'  - GET  {tunnel.public_url}/health')\n",
                "print(f'  - POST {tunnel.public_url}/analyze')\n",
                "print(f'\\nüîê Authentication: Bearer {os.environ[\"SOVEREIGN_ACCESS_TOKEN\"][:20]}...')\n",
                "print(f'\\n‚ö° Starting server (this will take ~2 minutes to load the model)...')\n",
                "print(f'=' * 60)\n",
                "\n",
                "# Add colab_brain to Python path\n",
                "sys.path.insert(0, '/content')\n",
                "\n",
                "# Import FastAPI app\n",
                "from colab_brain.server import app\n",
                "\n",
                "# Run server in Colab-compatible way\n",
                "import nest_asyncio\n",
                "nest_asyncio.apply()\n",
                "\n",
                "# Start uvicorn server\n",
                "import uvicorn\n",
                "config = uvicorn.Config(app, host='0.0.0.0', port=8000, log_level='info')\n",
                "server = uvicorn.Server(config)\n",
                "await server.serve()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚úÖ Testing (Optional)\n",
                "\n",
                "Run this in a **separate cell** while the server is running to test it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "\n",
                "# Replace with your tunnel URL from above\n",
                "TUNNEL_URL = 'https://your-tunnel-url.ngrok-free.dev'\n",
                "ACCESS_TOKEN = os.environ['SOVEREIGN_ACCESS_TOKEN']\n",
                "\n",
                "# Test health endpoint\n",
                "print('üè• Testing health endpoint...')\n",
                "response = requests.get(\n",
                "    f'{TUNNEL_URL}/health',\n",
                "    headers={'X-Sovereign-Token': ACCESS_TOKEN}\n",
                ")\n",
                "print(f'Status: {response.status_code}')\n",
                "print(f'Response: {response.json()}')\n",
                "\n",
                "print('\\n‚úÖ If you see status 200, the server is working!')\n",
                "print('   Your local Sovereign-Doc app can now use this URL for Vision Agent.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìù Notes\n",
                "\n",
                "- **Keep the server cell running** - don't interrupt it\n",
                "- **Free GPU limit**: Colab gives ~12 hours of T4 GPU time\n",
                "- **Restart needed**: If the model crashes, restart runtime and run all cells again\n",
                "- **Tunnel URL changes**: Each time you restart, you get a new ngrok URL\n",
                "\n",
                "## üõ†Ô∏è Troubleshooting\n",
                "\n",
                "**\"CUDA out of memory\"**\n",
                "- Restart runtime: Runtime ‚Üí Restart runtime\n",
                "- Make sure you selected T4 GPU, not CPU\n",
                "\n",
                "**\"ngrok authentication failed\"**\n",
                "- Check your NGROK_TOKEN in Colab Secrets\n",
                "- Get a new token from https://dashboard.ngrok.com\n",
                "\n",
                "**\"Model loading failed\"**\n",
                "- Check your HF_TOKEN in Colab Secrets\n",
                "- Verify you have access to Qwen models on HuggingFace"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}