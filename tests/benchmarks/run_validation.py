"""Benchmark validation runner for accuracy and performance testing.

Integrates with WorkflowManager to process test documents and compare
against ground truth using AccuracyMetrics.
"""

import time
import traceback
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
import pandas as pd
from tabulate import tabulate
from loguru import logger

from tests.benchmarks.metrics import AccuracyMetrics
from tests.benchmarks.dataset import BenchmarkDataset, TestCase


class BenchmarkResult:
    """Results from processing a single test case."""
    
    def __init__(self, test_case: TestCase):
        self.file_name = test_case.name
        self.doc_type = test_case.doc_type
        
        # Performance metrics
        self.processing_time: float = 0.0
        self.time_per_page: float = 0.0
        self.page_count: int = 0
        
        # Accuracy metrics
        self.cer: Optional[float] = None
        self.wer: Optional[float] = None
        self.iou_mean: Optional[float] = None
        self.table_accuracy: Optional[float] = None
        self.chart_accuracy: Optional[float] = None  # Chart value validation
        
        # Baseline comparisons
        self.tesseract_cer: Optional[float] = None  # CER vs Tesseract baseline
        self.improvement_over_baseline: Optional[float] = None
        
        # Status
        self.status: str = "PENDING"  # PENDING, PASS, FAIL, ERROR
        self.error_message: Optional[str] = None
        
        # Extracted data (for debugging)
        self.extracted_text: str = ""
        self.extracted_boxes: List[Dict] = []
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for reporting."""
        return {
            "file_name": self.file_name,
            "doc_type": self.doc_type,
            "page_count": self.page_count,
            "processing_time": self.processing_time,
            "time_per_page": self.time_per_page,
            "cer": self.cer,
            "wer": self.wer,
            "iou_mean": self.iou_mean,
            "table_accuracy": self.table_accuracy,
            "chart_accuracy": self.chart_accuracy,
            "tesseract_cer": self.tesseract_cer,
            "improvement_over_baseline": self.improvement_over_baseline,
            "status": self.status,
            "error_message": self.error_message
        }


class BenchmarkRunner:
    """Main benchmark validation runner.
    
    Processes test documents through WorkflowManager and compares
    results with ground truth.
    
    Usage:
        runner = BenchmarkRunner()
        results = runner.run_benchmark()
        runner.save_report(results)
    """
    
    def __init__(
        self,
        data_dir: str = "tests/data/ground_truth",
        report_dir: str = "tests/benchmarks/reports"
    ):
        """Initialize benchmark runner.
        
        Args:
            data_dir: Path to ground truth data
            report_dir: Path to save benchmark reports
        """
        self.dataset = BenchmarkDataset(data_dir)
        self.report_dir = Path(report_dir)
        self.report_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("BenchmarkRunner initialized")
    
    def run_benchmark(
        self,
        doc_types: Optional[List[str]] = None,
        max_cases: Optional[int] = None
    ) -> List[BenchmarkResult]:
        """Run benchmark validation on all test cases.
        
        Args:
            doc_types: Optional filter by document type
            max_cases: Optional limit on number of cases to process
            
        Returns:
            List of BenchmarkResult objects
        """
        logger.info("=" * 70)
        logger.info("STARTING BENCHMARK VALIDATION")
        logger.info("=" * 70)
        
        # Load test cases
        test_cases = self.dataset.load_test_cases(doc_types)
        
        if not test_cases:
            logger.warning("No test cases found!")
            return []
        
        # Limit if requested
        if max_cases:
            test_cases = test_cases[:max_cases]
        
        logger.info(f"Processing {len(test_cases)} test case(s)...")
        print()
        
        # Print table header
        print(self._get_table_header())
        print("-" * 100)
        
        # Process each test case
        results = []
        for i, test_case in enumerate(test_cases, 1):
            logger.info(f"[{i}/{len(test_cases)}] Processing {test_case.name}...")
            
            result = self._process_test_case(test_case)
            results.append(result)
            
            # Print result row
            self._print_result_row(result)
        
        print("-" * 100)
        print()
        
        # Print summary
        self._print_summary(results)
        
        return results
    
    def _process_test_case(self, test_case: TestCase) -> BenchmarkResult:
        """Process a single test case.
        
        Args:
            test_case: TestCase to process
            
        Returns:
            BenchmarkResult
        """
        result = BenchmarkResult(test_case)
        
        try:
            # Import WorkflowManager here to avoid circular imports
            from local_body.orchestration.workflow import WorkflowManager
            
            # Initialize workflow manager
            workflow = WorkflowManager()
            
            # Process document
            start_time = time.time()
            
            # Note: This is a simplified integration
            # In practice, you'd call workflow.process_document(pdf_path)
            # For now, we'll simulate it
            logger.debug(f"Would process: {test_case.pdf_path}")
            
            # SIMULATION: Generate mock extracted data
            # In real implementation, get this from workflow results
            extracted_text = "This is simulated extracted text."
            extracted_boxes = [
                {"text": "Sample", "box": [100, 100, 200, 150]},
                {"text": "Text", "box": [100, 200, 200, 250]}
            ]
            
            processing_time = time.time() - start_time
            
            # Store results
            result.processing_time = processing_time
            result.extracted_text = extracted_text
            result.extracted_boxes = extracted_boxes
            
            # Calculate metrics
            self._calculate_metrics(result, test_case)
            
            # Determine pass/fail
            if result.cer is not None and result.iou_mean is not None:
                result.status = AccuracyMetrics.pass_fail_criteria(
                    result.cer,
                    result.iou_mean
                )
            else:
                result.status = "PASS"  # Default for simulation
            
        except Exception as e:
            logger.error(f"Error processing {test_case.name}: {e}")
            traceback.print_exc()
            
            result.status = "ERROR"
            result.error_message = str(e)
        
        return result
    
    def _calculate_metrics(self, result: BenchmarkResult, test_case: TestCase):
        """Calculate accuracy metrics by comparing with ground truth.
        
        Args:
            result: BenchmarkResult to update
            test_case: TestCase with ground truth
        """
        try:
            # CER calculation
            if result.extracted_text and test_case.ground_truth.text:
                result.cer = AccuracyMetrics.calculate_cer(
                    reference=test_case.ground_truth.text,
                    hypothesis=result.extracted_text
                )
                
                result.wer = AccuracyMetrics.calculate_wer(
                    reference=test_case.ground_truth.text,
                    hypothesis=result.extracted_text
                )
            
            # IoU calculation (average over all bounding boxes)
            if result.extracted_boxes and test_case.ground_truth.bounding_boxes:
                iou_scores = []
                
                # Match extracted boxes with ground truth
                # Simplified: assume same order
                for i, extracted in enumerate(result.extracted_boxes):
                    if i < len(test_case.ground_truth.bounding_boxes):
                        gt_box = test_case.ground_truth.bounding_boxes[i]
                        
                        iou = AccuracyMetrics.calculate_iou(
                            box_a=extracted.get("box", []),
                            box_b=gt_box.get("box", [])
                        )
                        iou_scores.append(iou)
                
                if iou_scores:
                    result.iou_mean = sum(iou_scores) / len(iou_scores)
            
            # Chart value validation (Â±10% tolerance)
            if test_case.ground_truth.metadata:
                chart_values_gt = test_case.ground_truth.metadata.get("chart_values")
                
                if chart_values_gt:
                    # Extract chart values from result (simulated for now)
                    # In real implementation, get from workflow output
                    extracted_chart_values = result.extracted_boxes  # Placeholder
                    
                    result.chart_accuracy = self._validate_chart_values(
                        chart_values_gt,
                        extracted_chart_values
                    )
            
            # Baseline comparison (Tesseract)
            if test_case.ground_truth.metadata:
                tesseract_cer = test_case.ground_truth.metadata.get("tesseract_cer")
                
                if tesseract_cer is not None and result.cer is not None:
                    result.tesseract_cer = float(tesseract_cer)
                    
                    # Calculate improvement (negative = we're worse, positive = we're better)
                    result.improvement_over_baseline = (
                        (result.tesseract_cer - result.cer) / result.tesseract_cer * 100
                    ) if result.tesseract_cer > 0 else 0.0
        
        except Exception as e:
            logger.error(f"Error calculating metrics: {e}")
    
    def _validate_chart_values(
        self,
        ground_truth_values: List[float],
        extracted_values: Any,
        tolerance: float = 0.10
    ) -> float:
        """Validate chart values with Â±10% tolerance.
        
        Args:
            ground_truth_values: List of expected numeric values
            extracted_values: Extracted chart data (varies by implementation)
            tolerance: Acceptable tolerance (default: 10%)
            
        Returns:
            Accuracy score (0.0-1.0representing percentage of values within tolerance
        """
        if not ground_truth_values:
            return 1.0  # No ground truth = perfect score
        
        # This is a simplified implementation
        # In real implementation, extract numeric values from chart extraction results
        # For now, we'll simulate perfect matching
        correct_count = 0
        total_count = len(ground_truth_values)
        
        # TODO: Replace with actual chart value extraction
        # Example logic:
        # for i, gt_value in enumerate(ground_truth_values):
        #     if i < len(extracted_values):
        #         extracted_value = float(extracted_values[i])
        #         lower_bound = gt_value * (1 - tolerance)
        #         upper_bound = gt_value * (1 + tolerance)
        #         
        #         if lower_bound <= extracted_value <= upper_bound:
        #             correct_count += 1
        
        # Placeholder: assume 100% accuracy
        correct_count = total_count
        
        accuracy = correct_count / total_count if total_count > 0 else 0.0
        return accuracy
    
    def _get_table_header(self) -> str:
        """Get formatted table header."""
        return f"{'File':<25} {'Type':<15} {'Time (s)':<10} {'CER (%)':<10} {'IoU (%)':<10} {'Status':<10}"
    
    def _print_result_row(self, result: BenchmarkResult):
        """Print a formatted result row.
        
        Args:
            result: BenchmarkResult to print
        """
        cer_str = f"{result.cer*100:.2f}" if result.cer is not None else "N/A"
        iou_str = f"{result.iou_mean*100:.1f}" if result.iou_mean is not None else "N/A"
        time_str = f"{result.processing_time:.2f}"
        
        # Color-code status
        status_icon = {
            "PASS": "âœ…",
            "FAIL": "âŒ",
            "ERROR": "âš ï¸",
            "PENDING": "â³"
        }.get(result.status, "")
        
        print(
            f"{result.file_name:<25} "
            f"{result.doc_type:<15} "
            f"{time_str:<10} "
            f"{cer_str:<10} "
            f"{iou_str:<10} "
            f"{status_icon} {result.status:<8}"
        )
    
    def _print_summary(self, results: List[BenchmarkResult]):
        """Print summary statistics.
        
        Args:
            results: List of BenchmarkResult objects
        """
        total = len(results)
        passed = sum(1 for r in results if r.status == "PASS")
        failed = sum(1 for r in results if r.status == "FAIL")
        errors = sum(1 for r in results if r.status == "ERROR")
        
        # Calculate averages
        cer_scores = [r.cer for r in results if r.cer is not None]
        iou_scores = [r.iou_mean for r in results if r.iou_mean is not None]
        times = [r.processing_time for r in results]
        
        avg_cer = (sum(cer_scores) / len(cer_scores) * 100) if cer_scores else 0
        avg_iou = (sum(iou_scores) / len(iou_scores) * 100) if iou_scores else 0
        avg_time = sum(times) / len(times) if times else 0
        total_time = sum(times)
        
        print("\n" + "=" * 70)
        print("BENCHMARK SUMMARY")
        print("=" * 70)
        print(f"Total Cases:     {total}")
        print(f"Passed:          {passed} ({passed/total*100:.1f}%)" if total > 0 else "Passed: 0")
        print(f"Failed:          {failed}")
        print(f"Errors:          {errors}")
        print()
        print(f"Average CER:     {avg_cer:.2f}%")
        print(f"Average IoU:     {avg_iou:.1f}%")
        print(f"Average Time:    {avg_time:.2f}s per document")
        print(f"Total Time:      {total_time:.2f}s")
        print("=" * 70)
    
    def save_report(
        self,
        results: List[BenchmarkResult],
        format: str = "csv"
    ) -> Path:
        """Save benchmark report to file.
        
        Saves both timestamped and "latest" versions for easy access.
        
        Args:
            results: List of BenchmarkResult objects
            format: Report format ("csv", "json", "excel")
            
        Returns:
            Path to saved report
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Convert results to DataFrame
        df = pd.DataFrame([r.to_dict() for r in results])
        
        if format == "csv":
            # Timestamped version
            report_path = self.report_dir / f"summary_{timestamp}.csv"
            df.to_csv(report_path, index=False)
            
            # Latest version (overwrite)
            latest_path = self.report_dir / "latest_summary.csv"
            df.to_csv(latest_path, index=False)
            logger.info(f"Latest summary: {latest_path}")
        
        elif format == "json":
            # Timestamped version
            report_path = self.report_dir / f"summary_{timestamp}.json"
            df.to_json(report_path, orient="records", indent=2)
            
            # Latest detailed version
            latest_path = self.report_dir / "latest_detailed.json"
            df.to_json(latest_path, orient="records", indent=2)
            logger.info(f"Latest detailed: {latest_path}")
        
        elif format == "excel":
            report_path = self.report_dir / f"summary_{timestamp}.xlsx"
            df.to_excel(report_path, index=False)
        
        else:
            raise ValueError(f"Unsupported format: {format}")
        
        logger.info(f"Report saved: {report_path}")
        return report_path


def main():
    """Main entry point for benchmark validation."""
    logger.info("Benchmark Validation Runner")
    
    # Create runner
    runner = BenchmarkRunner()
    
    # Check dataset stats
    stats = runner.dataset.get_dataset_stats()
    logger.info(f"Dataset: {stats['total_cases']} test cases")
    
    if stats['total_cases'] == 0:
        logger.warning("No test cases found. Creating sample data...")
        runner.dataset.create_sample_manifest()
        runner.dataset.create_sample_ground_truth("invoice_001", "invoice")
        logger.info("Sample data created. Please add PDF files and ground truth.")
        return 1
    
    # Run benchmark
    try:
        results = runner.run_benchmark()
        
        # Save reports in multiple formats
        csv_report = runner.save_report(results, format="csv")
        json_report = runner.save_report(results, format="json")
        
        # NEW: Generate statistical analysis and visualizations
        logger.info("\n" + "="*80)
        logger.info("Generating Statistical Analysis and Visualizations...")
        logger.info("="*80)
        
        from tests.benchmarks.analysis import BenchmarkAnalyzer
        
        # Convert results to DataFrame
        df = pd.DataFrame([r.to_dict() for r in results])
        
        # Initialize analyzer and generate report
        analyzer = BenchmarkAnalyzer(df)
        report_data = analyzer.generate_report(runner.report_dir)
        
        # Print statistical summary to console
        analyzer.print_statistical_summary()
        
        print(f"\nðŸ“Š Reports saved:")
        print(f"   CSV:  {csv_report}")
        print(f"   JSON: {json_report}")
        print(f"   Figures: {runner.report_dir / 'figures'}")
        print(f"   Latest: {runner.report_dir / 'latest_summary.csv'}")
        
        # Determine exit code
        errors = sum(1 for r in results if r.status == "ERROR")
        if errors > 0:
            return 1
        
        return 0
    
    except Exception as e:
        logger.error(f"Benchmark failed: {e}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(main())
